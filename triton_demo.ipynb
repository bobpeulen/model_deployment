{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8caebd47",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6158b",
   "metadata": {},
   "source": [
    "<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Oracle_logo.svg/2560px-Oracle_logo.svg.png\" width=\"200\" align = \"left\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6713ca6",
   "metadata": {},
   "source": [
    "## **<h1 align =\"right\"><b> Oracle CloudWorld - Las Vegas</b></h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03238e2f",
   "metadata": {},
   "source": [
    "# **<h1 align =\"middle\"><b>NVIDIA Triton Inference Server, deploying GPT2</b></h1>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e6e72",
   "metadata": {},
   "source": [
    "Step by step instructions on deploying GPT2 using OCI Data Science, Model Deployment, on NVIDIA Triton Inference server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a613e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed604583",
   "metadata": {},
   "outputs": [],
   "source": [
    "## note. The below runs in Frankfurt. chance 'fra' in 'fra.ocir.io' to 'xx.ocir.io'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6336e4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conda installed and used: Tensforflow28_p38_gpu_v1\n",
    "\n",
    "# additional:\n",
    "#!pip install transformers tf2onnx\n",
    "#!pip install oracle-ads --upgrade\n",
    "#!pip install oci --upgrade\n",
    "#!pip install tensorflow --upgrade\n",
    "\n",
    "import os\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "import oci\n",
    "import ads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8d88e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736d2ec",
   "metadata": {},
   "source": [
    "# **1. GPT2 Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11c13d4",
   "metadata": {},
   "source": [
    "## **1.1 Run Model Downloader and Vocab Downloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf8fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\", from_pt=True, pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "model.save_pretrained(\"./gpt2model\", saved_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3799f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./vocab\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.save_vocabulary(\"./vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d26335",
   "metadata": {},
   "source": [
    "## **1.2 Transform tensorflow model to ONNX format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4360340",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ./converted_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"python -m tf2onnx.convert --saved-model ./gpt2model/saved_model/1 --opset 11  --output ./converted_output/model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33607820",
   "metadata": {},
   "source": [
    "## **1.3 Test, load and see the input and output layers of model.onnx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8332e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnx\n",
    "# from onnx_tf.backend import prepare\n",
    " \n",
    "# onnx_model = onnx.load(\"/home/datascience/4_ocw_las_vegas_triton/model.onnx\")\n",
    "# output = onnx_model.graph.output\n",
    "# input_all = onnx_model.graph.input\n",
    "\n",
    "# print(input_all)\n",
    "# print(\"######################################################################################\")\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba0197",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbd51a6",
   "metadata": {},
   "source": [
    "## **1.3 Clone Git repo and model artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d959dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/oracle-samples/oci-data-science-ai-samples.git\n",
    "!mv /home/datascience/4_ocw_las_vegas_triton/oci-data-science-ai-samples/model-deployment/containers/Triton/gpt2_ensemble/gpt-pipeline /home/datascience/4_ocw_las_vegas_triton/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fe789b",
   "metadata": {},
   "source": [
    "## **1.5 Copy merges.txt and config.json to folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e3da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ./vocab/merges.txt ./gpt-pipeline/encoder/1/\n",
    "!cp ./vocab/vocab.json ./gpt-pipeline/encoder/1/\n",
    "!cp ./vocab/merges.txt ./gpt-pipeline/decoder/1/\n",
    "!cp ./vocab/vocab.json ./gpt-pipeline/decoder/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab77db9",
   "metadata": {},
   "source": [
    "## **1.6 Make dir and copy model.onnx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12889e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir ./gpt-pipeline/gpt2/1\n",
    "!cp ./converted_output/model.onnx ./model_repository/gpt2/1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b991d0a0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b74ff4",
   "metadata": {},
   "source": [
    "# **2. Triton Inference Server**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e273500",
   "metadata": {},
   "source": [
    "## **2.1 Build Triton Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edac08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linux compute, download the private key and upload in directory here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Steps in terminal\n",
    "\n",
    "#change security of private key\n",
    "chmod 400 /home/datascience/4_ocw_las_vegas_triton/private_key.key\n",
    "\n",
    "#ssh into compute shape\n",
    "ssh -i /home/datascience/4_ocw_las_vegas_triton/private_key.key opc@89.168.91.125\n",
    "\n",
    "#install docker on compute\n",
    "sudo yum install docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd25301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit the ssh connection\n",
    "exit\n",
    "\n",
    "# copy docker file to compute\n",
    "scp -i /home/datascience/4_ocw_las_vegas_triton/private_key.key -pr /home/datascience/4_ocw_las_vegas_triton/Dockerfile opc@89.168.91.125:/home/opc\n",
    "scp -i /home/datascience/4_ocw_las_vegas_triton/private_key.key -pr /home/datascience/4_ocw_las_vegas_triton/entrypoint.sh opc@89.168.91.125:/home/opc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18eb06c",
   "metadata": {},
   "source": [
    "## **2. 2 Build Docker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6248ae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################## changed 1.0.0 to 1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy and run in terminal\n",
    "ssh -i /home/datascience/4_ocw_las_vegas_triton/private_key.key opc@89.168.91.125 \"docker build -t triton-server:1.1.0 . -f Dockerfile\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c18849",
   "metadata": {},
   "source": [
    "##### Output example:\n",
    "###### Successfully tagged localhost/triton-server:1.0.0\n",
    "###### ecdf0956040bf0a4b192d3ae072100adadc1e925260cf1be221d94dcb5740df5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234dbdf",
   "metadata": {},
   "source": [
    "## **2.3 Log in docker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c42846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Generate an Auth Token. Go to user settings in OCI, create Auth Token = password.\n",
    "## 2. User name to log in ocir is: <tenancy-namespace>/<username>. Example: oraseemeaanalytics/oracleidentitycloudservice/bob.peulen@oracle.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd98e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssh into compute\n",
    "ssh -i /home/datascience/4_ocw_las_vegas_triton/private_key.key opc@89.168.91.125\n",
    "\n",
    "## log in and enter credentials. Auth token and user name\n",
    "#docker login fra.ocir.io\n",
    "docker login -u 'frqap2zhtzbe/oracleidentitycloudservice/bob.peulen@oracle.com' --password 'ZI]jhAd]cNpllg9vQZCu' fra.ocir.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af6ddb",
   "metadata": {},
   "source": [
    "## **2.4 Create Container Registry**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd1d00",
   "metadata": {},
   "source": [
    "In OCI, go to Container Registry. Click \"Create Repository\". In the below, we used \"triton_inference_server\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912e240d",
   "metadata": {},
   "source": [
    "## **2.5 Tag Docker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5de6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker tag triton-server:1.1.0 fra.ocir.io/frqap2zhtzbe/nvidia_triton_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e891ba",
   "metadata": {},
   "source": [
    "## **2.6 Push Docker to OCIR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfe69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker push fra.ocir.io/frqap2zhtzbe/nvidia_triton_server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909f6ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77edf224",
   "metadata": {},
   "source": [
    "# **3. Store model in Model Catalog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename artifact folder to 'model_repository'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a88d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r artifacts_gpt2_v5.zip ./model_repository/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f74e3c8",
   "metadata": {},
   "source": [
    "# **4. Model Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0534ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## see steps here to deploy from UI: https://blogs.oracle.com/ai-and-datascience/post/oci-nvidia-triton-inference-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oci.data_science.models import UpdateModelConfigurationDetails, ModelConfigurationDetails, OcirModelDeploymentEnvironmentConfigurationDetails, CreateModelDeploymentDetails, InstanceConfiguration\n",
    "from oci import data_science\n",
    "# image_disgest you can find in the Container Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8ffcf1",
   "metadata": {},
   "source": [
    "## **4.1 Define Model Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb346c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = oci.config.from_file()\n",
    "\n",
    "# Initialize service client with default config file\n",
    "data_science_client = oci.data_science.DataScienceClient(config)\n",
    "\n",
    "instance = InstanceConfiguration(instance_shape_name = \"VM.GPU.A10.1\")\n",
    "\n",
    "# Create a model configuration details object\n",
    "model_config_details = ModelConfigurationDetails(\n",
    "    model_id= \"ocid1.datasciencemodel.oc1.eu-frankfurt-1.amaaaaaangencdyagh4jvam2sc7omvrwhtle2t47yhspvobfeecadivetrcq\",\n",
    "    instance_configuration = instance\n",
    ")\n",
    "\n",
    "# Create the container environment configuration\n",
    "environment_config_details = OcirModelDeploymentEnvironmentConfigurationDetails(\n",
    "    environment_configuration_type=\"OCIR_CONTAINER\",\n",
    "    environment_variables={'CONTAINER_TYPE': 'TRITON'},\n",
    "    image=\"fra.ocir.io/frqap2zhtzbe/triton_inference_server:latest\",\n",
    "    image_digest=\"sha256:ac88175fdc3e77db43cc382b65c1f93b242fa6d9947d074308714c0f2ddf9984\",\n",
    "    cmd=[\n",
    "        \"/entrypoint.sh\",\n",
    "        \"/opt/ds/model/deployed_model\"\n",
    "        \"None\",\n",
    "        \"5000\"\n",
    "    ],\n",
    "    server_port=5000,\n",
    "    health_check_port=5000\n",
    ")\n",
    "\n",
    "# create a model type deployment\n",
    "single_model_deployment_config_details = data_science.models.SingleModelDeploymentConfigurationDetails(\n",
    "    deployment_type=\"SINGLE_MODEL\",\n",
    "    model_configuration_details=model_config_details,\n",
    "    environment_configuration_details=environment_config_details\n",
    ")\n",
    "\n",
    "#logging\n",
    "\n",
    "\n",
    "# set up parameters required to create a new model deployment.\n",
    "create_model_deployment_details = CreateModelDeploymentDetails(    \n",
    "    display_name= \"gpt2_triton\",  \n",
    "    model_deployment_configuration_details = single_model_deployment_config_details,\n",
    "    compartment_id = \"ocid1.compartment.oc1..aaaaaaaae3n6r6hrjipbap2hojicrsvkzatrtlwvsyrpyjd7wjnw4za3m75q\",\n",
    "    project_id = \"ocid1.datascienceproject.oc1.eu-frankfurt-1.amaaaaaangencdyaik5ssdqk4as2bhldxprh7vnqpk7yycsm7vymd344cgua\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2de9c",
   "metadata": {},
   "source": [
    "## **4.2 Create model deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318358c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model deployment\n",
    "create_model_deployment_response = data_science_client.create_model_deployment(\n",
    "    create_model_deployment_details=create_model_deployment_details)\n",
    "print(create_model_deployment_response.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6005f0",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d27cc",
   "metadata": {},
   "source": [
    "# **5. Model Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac09eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"Machine learning is a field of computer science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a7d570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "Input Seq:: Machine learning is a field of computer science\n",
      "Out Seq:: Machine learning is a field of computer science that has been around for a long time . It\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer\n",
    "\n",
    "url = f\"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdyaevohj36x3qs4id5fcwhbehudd6kvpcipybxugq7gmzxa/predict\"\n",
    "\n",
    "\n",
    "config = oci.config.from_file(\"~/.oci/config\")\n",
    "auth = Signer(\n",
    "   tenancy=config['tenancy'],\n",
    "   user=config['user'],\n",
    "   fingerprint=config['fingerprint'],\n",
    "   private_key_file_location=config['key_file'],\n",
    "   pass_phrase=config['pass_phrase'])\n",
    "\n",
    "\n",
    "count = 0\n",
    "max_gen_len = 10\n",
    "gen_sentence = data\n",
    "\n",
    "while count < max_gen_len:\n",
    "    payload = {\n",
    "            \"inputs\": [\n",
    "                {\n",
    "                    \"name\": \"TEXT\",\n",
    "                    \"datatype\": \"BYTES\",\n",
    "                    \"shape\": [1],\n",
    "                    \"data\": [gen_sentence],\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    headers = {\"model_name\": \"ensemble_model\", \"model_version\": \"1\"}\n",
    "\n",
    "    ret = requests.post(\n",
    "            url,\n",
    "            json=payload,\n",
    "            auth=auth,\n",
    "            headers=headers\n",
    "        )\n",
    "\n",
    "    print(ret.status_code)\n",
    "    res = ret.json()\n",
    "    next_seq = str(res[\"outputs\"][0]['data'][0])\n",
    "    gen_sentence += \" \" + next_seq\n",
    "\n",
    "    count += 1\n",
    "\n",
    "print(\"Input Seq::\", data)\n",
    "print(\"Out Seq::\", gen_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6dd9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documentation\n",
    "# https://blogs.oracle.com/ai-and-datascience/post/llama2-oci-data-science-cloud-platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c57d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow28_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-tensorflow28_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
