{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07a2e9ee",
   "metadata": {},
   "source": [
    "# **Roberta Base - Deploying model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec324b",
   "metadata": {},
   "source": [
    "## **Load pre-trained model and save model locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32fd06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c526f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-offensive\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cafd4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "save_dir = \"./roberta_base_offensive_artifacts\"\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ac903",
   "metadata": {},
   "source": [
    "## **Load the model from local files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9caefb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"A model class to lead the model and tokenizer\"\"\"\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "  \n",
    "    def load_model():\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def load_tokenizer():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "        \n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1807c2df",
   "metadata": {},
   "source": [
    "## **Make prediction from locally loaded model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffeb6fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38024214 0.61975795]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "text = \"Good night  I really hate youðŸ˜Š\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff31a15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'offensive', 'score': 0.6198}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only top score. \n",
    "result = {}\n",
    "labels = [\"non-offensive\", \"offensive\"]\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "result[\"label\"] = str(labels[ranking[0]])\n",
    "result[\"score\"] = np.round(float(scores[ranking[0]]), 4)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3eeaf0",
   "metadata": {},
   "source": [
    "## **Local prediction again. Serialized as json input. Needed for when deployed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31993100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05765056 0.94234943]\n"
     ]
    }
   ],
   "source": [
    "#make post request\n",
    "data = {\"text\": \"Hi, Thanks, you are a bitch\"}\n",
    "\n",
    "#fetch text from input\n",
    "text_input = data['text']\n",
    "\n",
    "\n",
    "# same script again\n",
    "text = preprocess(text_input)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801ddb92",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca643ef",
   "metadata": {},
   "source": [
    "# **Create model artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "353e46d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ads.model.framework.tensorflow_model import TensorFlowModel\n",
    "from ads.common.model_metadata import UseCaseType\n",
    "from ads.common.model_artifact import ModelArtifact\n",
    "from ads.common.model_export_util import prepare_generic_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c734394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:ADS:As force_overwrite is set to True, all the existing files in the ./model_artifacts_roberta_base_offensive will be removed\n",
      "WARNING:ADS:Taxonomy metadata was not extracted. To auto-populate taxonomy metadata the model must be provided. Pass the model as a parameter to .prepare_generic_model(model=model, usecase_type=UseCaseType.REGRESSION). Alternative way is using atifact.populate_metadata(model=model, usecase_type=UseCaseType.REGRESSION).\n"
     ]
    }
   ],
   "source": [
    "#path to artifacts and conda slug\n",
    "path_to_artifacts = './model_artifacts_roberta_base_offensive'\n",
    "conda_env = 'oci://conda_environment_yolov5@frqap2zhtzbe/conda_environments/cpu/fdf_conda/1.0/fdf_conda'   #this refers to the published conda location (bucket name, namespace)\n",
    "\n",
    "#create default artifacts\n",
    "artifact = prepare_generic_model(\n",
    "    path_to_artifacts, \n",
    "    fn_artifact_files_included=False, \n",
    "    force_overwrite=True, \n",
    "    inference_conda_env=conda_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bda4f9",
   "metadata": {},
   "source": [
    "# **Change the score.py manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceb7d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "##########\n",
    "########## IMPORTANT. AFTER CREATING ARTIFACT FILESC HANGE PYTHON VERSION in the YAML file\n",
    "##########\n",
    "##########  ADD \"inference_python_version=3.7.0 to the above prepare_generic_model\n",
    "##########   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e74bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy all files in the model artifacts\n",
    "!cp -a ./roberta_base_offensive_artifacts ./model_artifacts_roberta_base_offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d7011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "###\n",
    "##  Output prediction is probably not entirely serialized? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42848bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile \"{path_to_artifacts}/score.py\"\n",
    "import os\n",
    "import ads\n",
    "from transformers import AutoModelForSequenceClassification, TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "import tokenizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from cloudpickle import cloudpickle\n",
    "from functools import lru_cache\n",
    "from scipy.special import softmax\n",
    "\n",
    "#load model and tokenizer\n",
    "model_artifacts_folder = \"./roberta_base_offensive_artifacts\"  \n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "    \n",
    "def load_model():\n",
    "    class DummyModel:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "    return DummyModel()\n",
    "\n",
    "#loading the model before seemed to fail. NOt sure why. Now loading in the predict.\n",
    "\n",
    "def predict(data, model=load_model()):\n",
    "    \n",
    "       \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_artifacts_folder)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_artifacts_folder)\n",
    "    \n",
    "    #fetch text from input\n",
    "    text_input = data['text']\n",
    "    \n",
    "    # process text\n",
    "    text = preprocess(text_input)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    encoded_input\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    #convert scores to json\n",
    "    scores_list = scores.tolist()\n",
    "    json_str = json.dumps(scores_list)\n",
    "    \n",
    "    return {'prediction': json_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6798821b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': '[0.8505518436431885, 0.14944812655448914]'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#smal test on local predict()\n",
    "\n",
    "data = {\"text\": \"Hi, Thanks, i'm very sad\"}\n",
    "\n",
    "predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803779e0",
   "metadata": {},
   "source": [
    "## **check the artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13ef1760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_json_output.json', '.ipynb_checkpoints', 'runtime.yaml', 'roberta_base_offensive_artifacts', 'score.py']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test key</th>\n",
       "      <th>Test name</th>\n",
       "      <th>Result</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>runtime_env_path</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>runtime_env_python</td>\n",
       "      <td>Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>runtime_path_exist</td>\n",
       "      <td>Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>runtime_version</td>\n",
       "      <td>Check that field MODEL_ARTIFACT_VERSION is set to 3.0</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>runtime_yaml</td>\n",
       "      <td>Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>score_load_model</td>\n",
       "      <td>Check that load_model() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>score_predict</td>\n",
       "      <td>Check that predict() is defined</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>score_predict_arg</td>\n",
       "      <td>Check that all other arguments in predict() are optional and have default values</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>score_predict_data</td>\n",
       "      <td>Check that the only required argument for predict() is named \"data\"</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>score_py</td>\n",
       "      <td>Check that the file \"score.py\" exists and is in the top level directory of the artifact directory</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>score_syntax</td>\n",
       "      <td>Check for Python syntax errors</td>\n",
       "      <td>Passed</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Test key  \\\n",
       "0     runtime_env_path   \n",
       "1   runtime_env_python   \n",
       "2   runtime_path_exist   \n",
       "3      runtime_version   \n",
       "4         runtime_yaml   \n",
       "5     score_load_model   \n",
       "6        score_predict   \n",
       "7    score_predict_arg   \n",
       "8   score_predict_data   \n",
       "9             score_py   \n",
       "10        score_syntax   \n",
       "\n",
       "                                                                                                Test name  \\\n",
       "0                                             Check that field MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is set   \n",
       "1           Check that field MODEL_DEPLOYMENT.INFERENCE_PYTHON_VERSION is set to a value of 3.6 or higher   \n",
       "2                             Check that the file path in MODEL_DEPLOYMENT.INFERENCE_ENV_PATH is correct.   \n",
       "3                                                   Check that field MODEL_ARTIFACT_VERSION is set to 3.0   \n",
       "4   Check that the file \"runtime.yaml\" exists and is in the top level directory of the artifact directory   \n",
       "5                                                                      Check that load_model() is defined   \n",
       "6                                                                         Check that predict() is defined   \n",
       "7                        Check that all other arguments in predict() are optional and have default values   \n",
       "8                                     Check that the only required argument for predict() is named \"data\"   \n",
       "9       Check that the file \"score.py\" exists and is in the top level directory of the artifact directory   \n",
       "10                                                                         Check for Python syntax errors   \n",
       "\n",
       "    Result Message  \n",
       "0   Passed          \n",
       "1   Passed          \n",
       "2   Passed          \n",
       "3   Passed          \n",
       "4   Passed          \n",
       "5   Passed          \n",
       "6   Passed          \n",
       "7   Passed          \n",
       "8   Passed          \n",
       "9   Passed          \n",
       "10  Passed          "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all should be passed\n",
    "artifact.introspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b8b163",
   "metadata": {},
   "source": [
    "## **Test model from artifacts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9d483d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, path_to_artifacts)\n",
    " \n",
    "# importing load_model() and predict() that are defined in score.py\n",
    "from score import load_model, predict\n",
    " \n",
    "# Loading the model to memory\n",
    "_ = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9427922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prediction': '[0.8946576118469238, 0.10534229874610901]'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"text\": \"Hi, Thanks, i'm very sad. You such\"}\n",
    "\n",
    "predictions_test = predict(data, _)\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8ec8a",
   "metadata": {},
   "source": [
    "## **store in catalog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22133229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "loop1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifact:/tmp/saved_model_0d92d2ca-2cc5-4595-a919-7390c01aaa90.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ocid1.datasciencemodel.oc1.eu-frankfurt-1.amaaaaaangencdyakalcl2h7go5opderxbcntx7jobk4lawdifc4i2pnu5ta'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model artifact to the model catalog. \n",
    "catalog_entry = artifact.save(display_name='roberta_base_offensive_v2', description='roberta_base_offensive_v2', timeout=600)\n",
    "catalog_entry.id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af241bb",
   "metadata": {},
   "source": [
    "## **Deploy in the UI**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819bd879",
   "metadata": {},
   "source": [
    "## **Invoke the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b38c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import oci\n",
    "from oci.signer import Signer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96a6ad83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdyacahzdoze6k2yrd3ywsdeyw65qewva2eqnzckabocgquq/predict\n"
     ]
    }
   ],
   "source": [
    "#fdf version mf_final\n",
    "uri = f\"https://modeldeployment.eu-frankfurt-1.oci.customer-oci.com/ocid1.datasciencemodeldeployment.oc1.eu-frankfurt-1.amaaaaaangencdyacahzdoze6k2yrd3ywsdeyw65qewva2eqnzckabocgquq/predict\"\n",
    "print(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a13eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Resource principal to authenticate against the model endpoint. Set using_rps=False if you are using the config+key flow. \n",
    "using_rps = False\n",
    "\n",
    "if using_rps: # using resource principal:     \n",
    "    auth = oci.auth.signers.get_resource_principals_signer()\n",
    "else: # using config + key: \n",
    "    config = oci.config.from_file(\"~/.oci/config\") # replace with the location of your oci config file\n",
    "    auth = Signer(\n",
    "        tenancy=config['tenancy'],\n",
    "        user=config['user'],\n",
    "        fingerprint=config['fingerprint'],\n",
    "        private_key_file_location=config['key_file'],\n",
    "        pass_phrase=config['pass_phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d1ab6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "{'prediction': '[0.25460579991340637, 0.7453941106796265]'}\n",
      "0.254\n",
      " 0.745\n",
      "CPU times: user 16.6 ms, sys: 5.34 ms, total: 21.9 ms\n",
      "Wall time: 1.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "#input data\n",
    "data = {\"text\": \"This person is very hatefull, I hate her\"}\n",
    "\n",
    "#POST request to the model\n",
    "response = requests.post(uri, json=data, auth=auth)\n",
    "print(response)\n",
    "xx = (json.loads(response.content))\n",
    "print(xx)\n",
    "print(xx['prediction'][1:6])\n",
    "offensive_value = (xx['prediction'][21:27])\n",
    "    \n",
    "print(offensive_value)\n",
    "\n",
    "\n",
    "#label 1 = non-offensive\n",
    "#Label 2 = offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464788e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1583d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fdf_conda]",
   "language": "python",
   "name": "conda-env-fdf_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
